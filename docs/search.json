[
  {
    "objectID": "UrinaryBiomarkers.html",
    "href": "UrinaryBiomarkers.html",
    "title": "Urinary Biomarkers for Pancreatic Cancer",
    "section": "",
    "text": "Authors: Katyna Sada, Cristina Tobías and Angélica Santos\nDate: March 2021"
  },
  {
    "objectID": "UrinaryBiomarkers.html#pancreatic-cancer",
    "href": "UrinaryBiomarkers.html#pancreatic-cancer",
    "title": "Urinary Biomarkers for Pancreatic Cancer",
    "section": "1. Pancreatic Cancer",
    "text": "1. Pancreatic Cancer\n\nPancreatic ductal adenocarcinoma (PDAC) is a highly fatal disease with a 5-year survival rate of approximtely 10% in the USA, and it is becoming an increasingly common cause of cancer mortality.\nHowever, with an early detection the odds of surviving are much better. Unfortunately, many cases of pacreatic cancer show no symptoms until the cancer has spread throughout the body.\nNew strategies for screening high-risk patients to detect pancreatic tumours at early stages are needed to make a clinically significant impact."
  },
  {
    "objectID": "UrinaryBiomarkers.html#objective",
    "href": "UrinaryBiomarkers.html#objective",
    "title": "Urinary Biomarkers for Pancreatic Cancer",
    "section": "2. Objective",
    "text": "2. Objective\nValidate a biomarker panel for earlier detection of PDAC in urine."
  },
  {
    "objectID": "UrinaryBiomarkers.html#dataset",
    "href": "UrinaryBiomarkers.html#dataset",
    "title": "Urinary Biomarkers for Pancreatic Cancer",
    "section": "3. Dataset",
    "text": "3. Dataset\n\nThe dataset evaluates 590 patients, of which, 183 are healthy individuals, 208 have a benign pancreatic tumor and the rest have pancreatic cancer in different stages.\nThe data has been obtained by performing 2 different studies (cohort 1 and cohort 2). In the first study, the biomarkers analysed were creatinine, LYVE1, REG1B, TFF1 and REG1A, whilst in the second, the last biomarker was not analysed.\nOther parameters that have been taken into account, besides the biomarkers, are the sex and age of the patients, the origin of the sample and, for the patients with a benign tumor, the symptoms.\n\n\n3.1 Loading libraries\n\n\nCode\n# Library import\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport matplotlib.ticker as ticker\nimport seaborn as sns\n\n# For creating the pipelines\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Algorithms\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n# For the metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n%matplotlib inline\n\n\n\n\n3.2 Data\n\nThe key features are the aforementioned urinary biomarkers: creatinine, LYVE1, REG1B and TFF1. * Creatinine: a protein that is often used as an indicator of kidney function. * YVLE1: lymphatic vessel endothelial hyaluronan receptor 1, a protein that may play a role in tumor metastasis. * REG1B: a protein that may be associated with pancreas regeneration * TFF1: trefoil factor 1, which may be related to regeneration and repair of the urinary tract.\n\n\n\nCode\ndata = pd.read_csv(\"DebernardiDataModificado.csv\")\ndata.head()\n\n\n\n\n\n\n  \n    \n      \n      sample_id\n      patient_cohort\n      sample_origin\n      age\n      sex\n      diagnosis\n      stage\n      benign_sample_diagnosis\n      plasma_CA19_9\n      creatinine\n      LYVE1\n      REG1B\n      T001\n      REG1A\n    \n  \n  \n    \n      0\n      S1\n      Cohort1\n      1\n      33\n      0\n      1\n      NaN\n      NaN\n      11.7\n      1.83222\n      0.893219\n      52.94884\n      654.282174\n      1262.000\n    \n    \n      1\n      S10\n      Cohort1\n      1\n      81\n      0\n      1\n      NaN\n      NaN\n      NaN\n      0.97266\n      2.037585\n      94.46703\n      209.488250\n      228.407\n    \n    \n      2\n      S100\n      Cohort2\n      1\n      51\n      1\n      1\n      NaN\n      NaN\n      7.0\n      0.78039\n      0.145589\n      102.36600\n      461.141000\n      NaN\n    \n    \n      3\n      S101\n      Cohort2\n      1\n      61\n      1\n      1\n      NaN\n      NaN\n      8.0\n      0.70122\n      0.002805\n      60.57900\n      142.950000\n      NaN\n    \n    \n      4\n      S102\n      Cohort2\n      1\n      62\n      1\n      1\n      NaN\n      NaN\n      9.0\n      0.21489\n      0.000860\n      65.54000\n      41.088000\n      NaN\n    \n  \n\n\n\n\n\nAs it can be seen in the matrix above, the variables stage and benign sample diagnosis are exclusive to a particular diagnosis. This can be used for further studies, nonetheless, in this project, those columns are going to be removed since the absent values cannot be imputed.\n\n\n\nCode\n# There are some features which are exclusive for the type of diagnosis.   \n# In case further studies have to be performed, that columns will be stored in these variables. \nstage = data.stage # Only for cancer patients\nbenign_sample_diagnosis = data.benign_sample_diagnosis # Only for benign tumors\n\n#We remove the unusable columns\ndata = data.drop(columns = ['sample_id','patient_cohort','stage','benign_sample_diagnosis'])\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n  \n    \n      \n      sample_origin\n      age\n      sex\n      diagnosis\n      plasma_CA19_9\n      creatinine\n      LYVE1\n      REG1B\n      T001\n      REG1A\n    \n  \n  \n    \n      0\n      1\n      33\n      0\n      1\n      11.7\n      1.83222\n      0.893219\n      52.94884\n      654.282174\n      1262.000\n    \n    \n      1\n      1\n      81\n      0\n      1\n      NaN\n      0.97266\n      2.037585\n      94.46703\n      209.488250\n      228.407\n    \n    \n      2\n      1\n      51\n      1\n      1\n      7.0\n      0.78039\n      0.145589\n      102.36600\n      461.141000\n      NaN\n    \n    \n      3\n      1\n      61\n      1\n      1\n      8.0\n      0.70122\n      0.002805\n      60.57900\n      142.950000\n      NaN\n    \n    \n      4\n      1\n      62\n      1\n      1\n      9.0\n      0.21489\n      0.000860\n      65.54000\n      41.088000\n      NaN\n    \n  \n\n\n\n\n\n\nCode\n# Define numerical and categorical features\nnumeric_cols = ['age', 'plasma_CA19_9','creatinine', 'LYVE1', 'REG1B', 'T001', 'REG1A']\ncat_cols = ['sample_origin','sex']\ndata[cat_cols] = data[cat_cols].astype('category')\ndata.diagnosis = data.diagnosis.astype('category')"
  },
  {
    "objectID": "UrinaryBiomarkers.html#exploratory-data-analysis",
    "href": "UrinaryBiomarkers.html#exploratory-data-analysis",
    "title": "Urinary Biomarkers for Pancreatic Cancer",
    "section": "4. Exploratory data analysis",
    "text": "4. Exploratory data analysis\n\nThe first step is to explore the data, this analysis is divided into different parts:\n\nIdentify missing values\nPlot the distributions of the numerical variables\nAnalyse the correlation between features\n\n\n\n\nCode\ndata.isna()\ndata.isna().sum() # Total 0s\n\n\nsample_origin      0\nage                0\nsex                0\ndiagnosis          0\nplasma_CA19_9    240\ncreatinine         0\nLYVE1              0\nREG1B              0\nT001               0\nREG1A            284\ndtype: int64\n\n\n\nAs stated before, there are several missing values for the plasma and the biomarker REG1A.\n\n\n\nCode\n# Distribution graphs for numerical variables\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(9, 5))\naxes = axes.flat\n\nfor i, colum in enumerate(numeric_cols):\n    sns.histplot(\n        data    = data,\n        x       = colum,\n        stat    = \"count\",\n        kde     = True,\n        palette   = sns.color_palette(\"muted\", 3),\n        line_kws= {'linewidth': 2},\n        alpha   = 0.3,\n        ax      = axes[i-2],\n        hue     = 'diagnosis'\n    )\n    axes[i-2].set_title(colum, fontsize = 7, fontweight = \"bold\")\n    axes[i-2].tick_params(labelsize = 6)\n    axes[i-2].set_xlabel(\"\")\n    \n    \nfig.tight_layout()\nplt.subplots_adjust(top = 0.9)\nfig.suptitle('Numerical variables distribution', fontsize = 10, fontweight = \"bold\");\n\n\n\n\n\nThe graphs show similar distributions among the diagnosis, therefore, a transformation will be applied afterwards in the models.\n\n\nCode\n# Boxplots for numerical variables\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(9, 5))\naxes = axes.flat\ncolumnas_numeric = data.select_dtypes(include=['float64', 'int']).columns\n\nfor i, colum in enumerate(numeric_cols):\n    sns.boxplot(\n        data    = data,\n        y       = colum,\n        ax      = axes[i-2],\n        x       = 'diagnosis',\n    )\n    axes[i-2].set_title(colum, fontsize = 7, fontweight = \"bold\")\n    axes[i-2].tick_params(labelsize = 6)\n    axes[i-2].set_xlabel(\"\")\n    \n    \n    \nfig.tight_layout()\nplt.subplots_adjust(top = 0.9)\nfig.suptitle('Numerical variables boxplot', fontsize = 10, fontweight = \"bold\");\n\n\n\n\n\n\n\nCode\n# Boxplot of age\nsns.boxplot(x=\"diagnosis\", y=\"age\", data=data) \n\n\n<AxesSubplot:xlabel='diagnosis', ylabel='age'>\n\n\n\n\n\nIt is important to know the distribuition of all variables taken into account in the models, in this case, all the diagnosis have a similar number of men and women. The difference in age range is also not that significant, however the age range on the cancer pantients is higher.\n\n\nCode\n# Sex based on diagnosis\nsns.countplot(x ='diagnosis', hue = \"sex\", data = data)\n\n\n<AxesSubplot:xlabel='diagnosis', ylabel='count'>\n\n\n\n\n\n\n\nCode\n# Sample_origin based on diagnosis\nsns.countplot(x ='diagnosis', hue = \"sample_origin\", data = data)\n\n\n<AxesSubplot:xlabel='diagnosis', ylabel='count'>\n\n\n\n\n\nThis is not the case for the sample origin, since most of the healthy samples come from the first hospital and that all of the samples from the 4th hospital have a benign tumor.\n\n\nCode\n\nsns.pairplot(data, hue = 'diagnosis', markers=[\"o\", \"s\", \"D\"], palette= sns.color_palette(\"muted\", 3), corner=True)\n\n\n<seaborn.axisgrid.PairGrid at 0x7f8c5ac014f0>\n\n\n\n\n\nThere is barely correlation between the features, the diagnosis does not seem to have an effect neither.\nAfter considering the plots, it is clear that the models will have suboptimal results when predicting 3 diagnosis. The proposed solution to this problem is to create two different types of models. The first type will try to predict all 3 diagnosis, whilst the second will only predict whether a patient is healthy or not."
  },
  {
    "objectID": "UrinaryBiomarkers.html#models-with-3-outcomes",
    "href": "UrinaryBiomarkers.html#models-with-3-outcomes",
    "title": "Urinary Biomarkers for Pancreatic Cancer",
    "section": "5. Models with 3 outcomes",
    "text": "5. Models with 3 outcomes\nFor these models we will considered three possible outcomes: * 1: Healthy controls. * 2: Patients with non-cancerous pancreatic conditions, like chronic pancreatitis. * 3: Patients with pancreatic ductal adenocarcinoma.\n\n5.1 Division of data\nStratified sampling was used in order to guarantee the same proportion in each class than the one had in the complete data set.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\nX = data.drop(columns='diagnosis') \ny = data[['diagnosis']]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42,stratify=y)\nX_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n\n\n\n\n5.2 Preprocessing\nAs stated before, transformations ought to be applied to the data in order to analyse it. The chosen transformation has been the logarithmic transform, since this operation stretches the distribution of numerical variables.\n\n\nCode\ndef log_transform(x):\n    return np.log(x + 1) # required for applying the logarithm to the numeric variables\n\ntransformer = FunctionTransformer(log_transform)\nscaler = StandardScaler()\n\n\n\n\nCode\n # Boxplots of data when applying the transform\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(9, 5))\naxes = axes.flat\ncolumnas_numeric = data.select_dtypes(include=['float64', 'int']).columns\n\nfor i, colum in enumerate(numeric_cols):\n    sns.boxplot(\n        data    = data,\n        y       = (transformer.transform(data[colum])),\n        ax      = axes[i-2],\n        x       = 'diagnosis',\n    )\n    axes[i-2].set_title(colum, fontsize = 7, fontweight = \"bold\")\n    axes[i-2].tick_params(labelsize = 6)\n    axes[i-2].set_xlabel(\"\")\n    \n    \nfig.tight_layout()\nplt.subplots_adjust(top = 0.9)\nfig.suptitle('Logarithm of numerical variables boxplot', fontsize = 10, fontweight = \"bold\");\n#\n\n\n\n\n\n\n\nCode\nplotting_data = transformer.transform(data.select_dtypes(include=['float64', 'int']))\nplotting_data = pd.concat([plotting_data.reset_index(drop=True), data.diagnosis], axis=1)\n\nsns.pairplot(plotting_data, hue = 'diagnosis', markers=[\"o\", \"s\", \"D\"], palette= sns.color_palette(\"muted\", 3), corner=True)\n\n\n<seaborn.axisgrid.PairGrid at 0x7f8c5aed39a0>\n\n\n\n\n\nA more significant difference can be observed between classes when applying the log transform. Furthermore, some variables seem to be correlated in this scale.\nAfter checking the effectiveness of the transformation, the pipeline is created. All the numerical variables will undergo the previous transformation, moreover, the categorical variables will be encoded so that the models can take them into account.\nThe missing values will be imputed by the means of a KNN imputer in both cases.\n\n\nCode\n# Numerical variables\nnumeric_transformer = Pipeline(\n                        steps=[('transformer',transformer), # Log\n                               ('imputer', KNNImputer()), #Imputation for completing missing values using k-Nearest Neighbors.\n                               ('scaler', scaler)]) # Standardize features by removing the mean and scaling to unit variance\n\n# Categorical variables\ncategorical_transformer = Pipeline(\n                            steps=[('imputer', KNNImputer()),\n                                   ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Preprocessor\npreprocessor = ColumnTransformer(\n                    transformers=[\n                        ('cat', categorical_transformer, cat_cols),\n                        ('numeric', numeric_transformer, numeric_cols)],\n                        remainder='passthrough')\n\n\n\n\n5.3 Predictive models\nDifferent predictive models were first validated with the default tunning parameters.\n\n\nCode\nkey = ['KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier']\nvalue = [KNeighborsClassifier(weights ='distance'), SVC(random_state=42), DecisionTreeClassifier(random_state=42), RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier()]\nmodels = dict(zip(key,value))\nmodels\n\npredicted =[]\nfor name,algo in models.items():\n    model=algo\n    pipe = Pipeline([('preprocessing', preprocessor),  ('modelo', model)])\n    pipe.fit(X=X_train, y=y_train)\n    predict = pipe.predict(X_val)\n    acc = accuracy_score(y_val, predict)\n    predicted.append(acc)\n    \n    \nplt.figure(figsize = (10,5))\nsns.barplot(x = predicted, y = key, palette='muted')\n\n\n/Users/katyna/opt/anaconda3/envs/SparseGoNew/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  return self._fit(X, y)\n/Users/katyna/opt/anaconda3/envs/SparseGoNew/lib/python3.8/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/katyna/opt/anaconda3/envs/SparseGoNew/lib/python3.8/site-packages/sklearn/pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n/Users/katyna/opt/anaconda3/envs/SparseGoNew/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/Users/katyna/opt/anaconda3/envs/SparseGoNew/lib/python3.8/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nCode\npredictions = dict(zip(key,predicted))\nprint(predictions)\n\n\n{'KNeighborsClassifier': 0.6565656565656566, 'SVC': 0.5959595959595959, 'DecisionTreeClassifier': 0.6464646464646465, 'RandomForestClassifier': 0.696969696969697, 'GradientBoostingClassifier': 0.696969696969697, 'AdaBoostClassifier': 0.5858585858585859}\n\n\nThe best predictive models are Random Forest and GradientBoostingClassifier. The chosen model to tune was GradientBoostingClassifier.\n\n\n5.4 Tunning the chosen model\n\nGradient boosting\n\n\nCode\ntuned_parameters = [ {'modelo__loss': \n                      ['deviance', 'exponential'], \n                      'modelo__learning_rate': [0.1, 0.01, 0.001, 0.0001],\n                      'modelo__subsample': [0.1, 0.5, 1, 2],\n                      'modelo__criterion': ['friedman_mse', 'mse', 'mae']}\n                   ]\n\npipe = Pipeline([('preprocessing', preprocessor),\n                 ('modelo', GradientBoostingClassifier(random_state=42))]) \n\nfrom sklearn.model_selection import GridSearchCV\n\nclf = GridSearchCV(pipe,tuned_parameters,refit=True,verbose=2, cv = 5, \n                     scoring='accuracy')\nimport warnings\nwarnings.filterwarnings('ignore') \nclf.fit(X_train, y_train)\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\n\n\n\n\nCode\nresults = pd.DataFrame(clf.cv_results_)\nresults.filter(regex = '(param.*|mean_t|std_t)')\\\n    .drop(columns = 'params')\\\n    .sort_values('mean_test_score', ascending = False)\n\n\n\n\n\n\n  \n    \n      \n      param_modelo__criterion\n      param_modelo__learning_rate\n      param_modelo__loss\n      param_modelo__subsample\n      mean_test_score\n      std_test_score\n    \n  \n  \n    \n      1\n      friedman_mse\n      0.1\n      deviance\n      0.5\n      0.669209\n      0.061035\n    \n    \n      33\n      mse\n      0.1\n      deviance\n      0.5\n      0.665819\n      0.053922\n    \n    \n      34\n      mse\n      0.1\n      deviance\n      1\n      0.655424\n      0.012859\n    \n    \n      2\n      friedman_mse\n      0.1\n      deviance\n      1\n      0.655424\n      0.012859\n    \n    \n      9\n      friedman_mse\n      0.01\n      deviance\n      0.5\n      0.631864\n      0.036484\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      91\n      mae\n      0.0001\n      deviance\n      2\n      NaN\n      NaN\n    \n    \n      92\n      mae\n      0.0001\n      exponential\n      0.1\n      NaN\n      NaN\n    \n    \n      93\n      mae\n      0.0001\n      exponential\n      0.5\n      NaN\n      NaN\n    \n    \n      94\n      mae\n      0.0001\n      exponential\n      1\n      NaN\n      NaN\n    \n    \n      95\n      mae\n      0.0001\n      exponential\n      2\n      NaN\n      NaN\n    \n  \n\n96 rows × 6 columns\n\n\n\nThe best parameters found based on the accuracy are shown below.\n\n\nCode\nclf.best_params_\n\n\n{'modelo__criterion': 'friedman_mse',\n 'modelo__learning_rate': 0.1,\n 'modelo__loss': 'deviance',\n 'modelo__subsample': 0.5}\n\n\nBefore the prediction, a new validation will be performed, so as to check whether the new model has indeed improved with respect to the previous one.\n\n\nCode\nnew_val = clf.predict(X_val)\nprint(accuracy_score(y_val, new_val))\n\n\n0.696969696969697\n\n\nFinally, the tuned model is tested.\n\n\nCode\nprediction = clf.predict(X_test)\n\n\n\n\nCode\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint('Confusion matrix for 3 diagnosis: ')\nprint(' ')\nprint(confusion_matrix(y_test,prediction))\nprint(' ')\nprint(classification_report(y_test,prediction))\nprint(' ')\nprint('Accuracy of the model: ', accuracy_score(y_test, prediction))\n\n\nConfusion matrix for 3 diagnosis: \n \n[[44 13  3]\n [18 36 15]\n [ 3  5 58]]\n \n              precision    recall  f1-score   support\n\n           1       0.68      0.73      0.70        60\n           2       0.67      0.52      0.59        69\n           3       0.76      0.88      0.82        66\n\n    accuracy                           0.71       195\n   macro avg       0.70      0.71      0.70       195\nweighted avg       0.70      0.71      0.70       195\n\n \nAccuracy of the model:  0.7076923076923077"
  },
  {
    "objectID": "UrinaryBiomarkers.html#models-with-2-outcomes",
    "href": "UrinaryBiomarkers.html#models-with-2-outcomes",
    "title": "Urinary Biomarkers for Pancreatic Cancer",
    "section": "6. Models with 2 outcomes",
    "text": "6. Models with 2 outcomes\nFor these models we will only consider two possible outcomes: PADC or no-PADC. For this, the diagnosis data of healthy (1) and non-cancerous pancreas condition (2) will be merged.\n\n\nCode\nnew_data = data\nnew_data.diagnosis[new_data.diagnosis == 2] = 1\nnew_data.diagnosis[new_data.diagnosis == 3] = 2\nnew_data.diagnosis = new_data.diagnosis.cat.remove_unused_categories()\n\n\n\n\nCode\nnew_data.diagnosis\n\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n585    2\n586    2\n587    2\n588    2\n589    2\nName: diagnosis, Length: 590, dtype: category\nCategories (2, int64): [1, 2]\n\n\nA new exploratory analysis can be performed to assess whether this study will have a more positive result than the previous one.\nThe boxplots below show a more significant difference than the ones where 3 diagnosis were being evaluated. Nonetheless, it is still advisable to apply a logarithmic transform.\n\n\nCode\n# Boxplots for numerical variables\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(9, 5))\naxes = axes.flat\ncolumnas_numeric = new_data.select_dtypes(include=['float64', 'int']).columns\n\nfor i, colum in enumerate(numeric_cols):\n    sns.boxplot(\n        data    = new_data,\n        y       = colum,\n        ax      = axes[i-2],\n        x       = 'diagnosis',\n    )\n    axes[i-2].set_title(colum, fontsize = 7, fontweight = \"bold\")\n    axes[i-2].tick_params(labelsize = 6)\n    axes[i-2].set_xlabel(\"\")\n    \n    \n    \nfig.tight_layout()\nplt.subplots_adjust(top = 0.9)\nfig.suptitle('Numerical variables boxplot', fontsize = 10, fontweight = \"bold\");\n\n\n\n\n\nAs for the densities and correlations, this separation of the data allows a better visualization of clusters.\n\n\nCode\nsns.pairplot(new_data, hue = 'diagnosis', markers=[\"o\", \"s\"], palette= sns.color_palette(\"muted\", 2), corner=True)\n\n\n<seaborn.axisgrid.PairGrid at 0x7f8c489db400>\n\n\n\n\n\nThe same graphs are evaluated after the application of a log transform and the results show clear difference between the two groups.\n\n\nCode\n # Boxplots of data when applying the transform\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(9, 5))\naxes = axes.flat\ncolumnas_numeric = new_data.select_dtypes(include=['float64', 'int']).columns\n\nfor i, colum in enumerate(numeric_cols):\n    sns.boxplot(\n        data    = data,\n        y       = (transformer.transform(new_data[colum])),\n        ax      = axes[i-2],\n        x       = 'diagnosis',\n    )\n    axes[i-2].set_title(colum, fontsize = 7, fontweight = \"bold\")\n    axes[i-2].tick_params(labelsize = 6)\n    axes[i-2].set_xlabel(\"\")\n    \n    \nfig.tight_layout()\nplt.subplots_adjust(top = 0.9)\nfig.suptitle('Logarithm of numerical variables boxplot', fontsize = 10, fontweight = \"bold\");\n#\n\n\n\n\n\n\n\nCode\nplotting_data = transformer.transform(new_data.select_dtypes(include=['float64', 'int']))\nplotting_data = pd.concat([plotting_data.reset_index(drop=True), new_data.diagnosis], axis=1)\nsns.pairplot(plotting_data, hue = 'diagnosis', markers=[\"o\", \"s\"], palette= sns.color_palette(\"muted\", 2), corner=True)\n\n\n<seaborn.axisgrid.PairGrid at 0x7f8c3c8b9a00>\n\n\n\n\n\n\n\nCode\ny2class = new_data.diagnosis\n\n\n\n6.1 Division of data\nThe new data will be divided into train, validation and test using the same method as before.\n\n\nCode\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X,y2class, test_size = 0.33, random_state = 42,stratify=y2class)\nX_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size=0.25, random_state=1)\n\n\n\n\n6.2 Predictive models\nThe same models as before are validated, the tunning parameters are the ones set by default.\n\n\nCode\nkey2 = ['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier']\nvalue2 = [LogisticRegression(),KNeighborsClassifier(weights ='distance'), SVC(random_state=42), DecisionTreeClassifier(random_state=42), RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier()]\nmodels2 = dict(zip(key2,value2))\nmodels2\n\npredicted2 =[]\nfor name,algo in models2.items():\n    model=algo\n    pipe = Pipeline([('preprocessing', preprocessor),  ('modelo', model)])\n    pipe.fit(X=X_train2, y=y_train2)\n    predict2 = pipe.predict(X_val2)\n    acc = accuracy_score(y_val2, predict2)\n    predicted2.append(acc)\n    \n    \nplt.figure(figsize = (10,5))\nsns.barplot(x = predicted2, y = key2, palette='muted')\n\n\n<AxesSubplot:>\n\n\n\n\n\nAs expected, these models have significantly improved their results in this scenario.\n\n\nCode\npredictions2 = dict(zip(key2,predicted2))\nprint(predictions2)\n\n\n{'LogisticRegression': 0.8383838383838383, 'KNeighborsClassifier': 0.8585858585858586, 'SVC': 0.8888888888888888, 'DecisionTreeClassifier': 0.8282828282828283, 'RandomForestClassifier': 0.8888888888888888, 'GradientBoostingClassifier': 0.8686868686868687, 'AdaBoostClassifier': 0.8484848484848485}\n\n\nThe best predictors are the SVC and the RandomForestClassfier. The chosen model to tune is the RF.\n\n\n6.3 Tunning the chosen two class model\n\nRandom Forest Classifier\n\n\nCode\nfrom sklearn.model_selection import RandomizedSearchCV, RepeatedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport multiprocessing\n\n# Se combinan los pasos de preprocesado y el modelo en un mismo pipeline.\npipe = Pipeline([('preprocessing', preprocessor),\n                 ('modelo', RandomForestClassifier())])\n\n# Hyperparameters optimization\n\nparam_distributions = {\n    'modelo__n_estimators': [20, 30, 50, 100, 150, 200],\n    'modelo__criterion': [\"gini\", \"entropy\"],\n    'modelo__max_features': [\"auto\", 3, 5, 7],\n    'modelo__max_depth'   : [3, 5, 10, 20]\n}\n\n# Random grid search\ngrid = RandomizedSearchCV(\n        estimator  = pipe,\n        param_distributions = param_distributions,\n        n_iter     = 20,\n        scoring    = 'accuracy',\n        n_jobs     = multiprocessing.cpu_count() - 1,\n        cv         = RepeatedKFold(n_splits = 5, n_repeats = 3),\n        refit      = True, \n        verbose    = 2,\n        random_state = 123,\n        return_train_score = True\n       )\n\ngrid.fit(X = X_train2, y = y_train2)\n\n\nFitting 15 folds for each of 20 candidates, totalling 300 fits\n\n\nRandomizedSearchCV(cv=RepeatedKFold(n_repeats=3, n_splits=5, random_state=None),\n                   estimator=Pipeline(steps=[('preprocessing',\n                                              ColumnTransformer(remainder='passthrough',\n                                                                transformers=[('cat',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                KNNImputer()),\n                                                                                               ('onehot',\n                                                                                                OneHotEncoder(handle_unknown='ignore'))]),\n                                                                               ['sample_origin',\n                                                                                'sex']),\n                                                                              ('numeric',\n                                                                               Pipeline(steps=[('transformer',\n                                                                                                Functi...\n                                                                                'creatinine',\n                                                                                'LYVE1',\n                                                                                'REG1B',\n                                                                                'T001',\n                                                                                'REG1A'])])),\n                                             ('modelo',\n                                              RandomForestClassifier())]),\n                   n_iter=20, n_jobs=7,\n                   param_distributions={'modelo__criterion': ['gini',\n                                                              'entropy'],\n                                        'modelo__max_depth': [3, 5, 10, 20],\n                                        'modelo__max_features': ['auto', 3, 5,\n                                                                 7],\n                                        'modelo__n_estimators': [20, 30, 50,\n                                                                 100, 150,\n                                                                 200]},\n                   random_state=123, return_train_score=True,\n                   scoring='accuracy', verbose=2)\n\n\n\n\nCode\n#Parameters chosen\ngrid.best_params_\n\n\n{'modelo__n_estimators': 200,\n 'modelo__max_features': 5,\n 'modelo__max_depth': 5,\n 'modelo__criterion': 'gini'}\n\n\n\n\nCode\n#Check the validation again\nnew_val2 = grid.predict(X_val2)\nprint(accuracy_score(y_val2, new_val2))\n\n\n0.8686868686868687\n\n\n\n\nCode\n#See the results in the test data\nprediction = grid.predict(X_test2)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test2,prediction))\nprint(classification_report(y_test2,prediction))\n\n\n[[120   9]\n [  8  58]]\n              precision    recall  f1-score   support\n\n           1       0.94      0.93      0.93       129\n           2       0.87      0.88      0.87        66\n\n    accuracy                           0.91       195\n   macro avg       0.90      0.90      0.90       195\nweighted avg       0.91      0.91      0.91       195"
  },
  {
    "objectID": "UrinaryBiomarkers.html#conclusions",
    "href": "UrinaryBiomarkers.html#conclusions",
    "title": "Urinary Biomarkers for Pancreatic Cancer",
    "section": "7. Conclusions",
    "text": "7. Conclusions\n\nThe tunned models show a high performance in both cases, nevertheless, due to the similarities between groups in the first case, a second analysis was performed in order to see if better predictions could be obtained.\nThe best model was the random forest classifier, which was built for only two classes. It uses the gini criterion, a maximum depth of 5 and 50 estimators. The accuracy obtained on the test set was 0,89.\nThese results show that the analysed biomarkers can accurately predict whether a patient has cancer or not.\nFuture lines of research can include the prediction of symptoms for benign tumors or the prediction of the stage of the cancer in the case of a positive diagnosis."
  }
]